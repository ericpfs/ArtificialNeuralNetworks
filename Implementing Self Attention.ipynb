{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c2519f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention outputs are\n",
      " [[ 0.12928972  0.56180036  1.44882858 ... -1.74847076 -1.55449529\n",
      "  -5.00044474]\n",
      " [ 0.12928972  0.56180036  1.44882858 ... -1.74847076 -1.55449529\n",
      "  -5.00044474]\n",
      " [ 0.12928972  0.56180036  1.44882858 ... -1.74847076 -1.55449529\n",
      "  -5.00044474]\n",
      " ...\n",
      " [ 0.12928972  0.56180036  1.44882858 ... -1.74847076 -1.55449529\n",
      "  -5.00044474]\n",
      " [ 0.12928972  0.56180036  1.44882858 ... -1.74847076 -1.55449529\n",
      "  -5.00044474]\n",
      " [ 0.12928972  0.56180036  1.44882858 ... -1.74847076 -1.55449529\n",
      "  -5.00044474]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "\n",
    "# I. Define the input data X\n",
    "# X consists out of 32 samples, each sample has dimensionality 256\n",
    "n = 32\n",
    "d = 256\n",
    "X = randn(n, d) # (32, 256)\n",
    "\n",
    "# II. Generate the projection weights\n",
    "Wq = randn(d, d) #(256, 256)\n",
    "Wk = randn(d, d)\n",
    "Wv = randn(d, d)\n",
    "\n",
    "# III. Project X to find its query, keys and values vectors\n",
    "Q = np.dot(X, Wq) # (32, 256)\n",
    "K = np.dot(X, Wk)\n",
    "V = np.dot(X, Wv)\n",
    "\n",
    "# IV. Compute the self-attention score, denoted by A\n",
    "# A = softmax(QK^T / \\sqrt{d})\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    z = np.clip(z, 100, -100) # clip in case softmax explodes\n",
    "    tmp = np.exp(z)\n",
    "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
    "    return res\n",
    "\n",
    "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
    "\n",
    "# V. Compute the self-attention output\n",
    "# outputs = A * V\n",
    "outputs = np.dot(A, V) #(32, 256)\n",
    "\n",
    "print(\"The attention outputs are\\n {}\".format(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a958e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:torch.Size([32, 20, 128]) \n",
      " Q.shape:torch.Size([32, 20, 64]) \n",
      " K.shape:torch.Size([32, 20, 64]) \n",
      " V.shape:torch.Size([32, 20, 32])\n",
      "attention matrix:  torch.Size([32, 20, 20])\n",
      "attention outputs:  torch.Size([32, 20, 32])\n",
      "tensor([[[ 0.1804,  0.1362,  0.1774,  ...,  0.2700,  0.2799, -0.2076],\n",
      "         [ 0.1551,  0.0536,  0.1839,  ...,  0.2479,  0.2560, -0.1974],\n",
      "         [ 0.0705,  0.0541,  0.1718,  ...,  0.1769,  0.2233, -0.2124],\n",
      "         ...,\n",
      "         [ 0.0950,  0.0392,  0.1534,  ...,  0.2462,  0.2793, -0.2168],\n",
      "         [ 0.1311,  0.1044,  0.2314,  ...,  0.2689,  0.2349, -0.1613],\n",
      "         [ 0.1274,  0.0536,  0.1555,  ...,  0.2373,  0.2401, -0.1581]],\n",
      "\n",
      "        [[ 0.1901,  0.0841, -0.0561,  ...,  0.2223,  0.0370,  0.0209],\n",
      "         [ 0.1805,  0.0632, -0.1261,  ...,  0.2702,  0.0404,  0.0188],\n",
      "         [ 0.1816,  0.0806, -0.0515,  ...,  0.2256,  0.0554,  0.0304],\n",
      "         ...,\n",
      "         [ 0.2062,  0.0815, -0.0661,  ...,  0.2092,  0.0722, -0.0224],\n",
      "         [ 0.1423,  0.0710, -0.0419,  ...,  0.2012, -0.0294,  0.0834],\n",
      "         [ 0.1531,  0.0417, -0.0180,  ...,  0.2355,  0.0004,  0.0095]],\n",
      "\n",
      "        [[ 0.0622,  0.1458,  0.0789,  ...,  0.0406,  0.1298, -0.1540],\n",
      "         [ 0.1030,  0.1453,  0.1685,  ...,  0.0908,  0.0282, -0.2590],\n",
      "         [ 0.0738,  0.1046,  0.1081,  ...,  0.0344,  0.1104, -0.0993],\n",
      "         ...,\n",
      "         [ 0.0842,  0.0938,  0.1022,  ...,  0.0270,  0.1255, -0.1014],\n",
      "         [ 0.1405,  0.1289,  0.1548,  ...,  0.0235,  0.1445, -0.2357],\n",
      "         [ 0.0631,  0.1027,  0.0738,  ...,  0.0870,  0.1225, -0.1345]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0646,  0.0187, -0.0080,  ..., -0.2285, -0.2239,  0.0859],\n",
      "         [ 0.0428,  0.0117, -0.0630,  ..., -0.1562, -0.2574,  0.0763],\n",
      "         [ 0.0378, -0.0052,  0.0964,  ..., -0.0840, -0.1992,  0.0240],\n",
      "         ...,\n",
      "         [ 0.0076, -0.0444,  0.0930,  ..., -0.1139, -0.2571,  0.0302],\n",
      "         [-0.0118,  0.0684,  0.0676,  ..., -0.1028, -0.2129, -0.0448],\n",
      "         [ 0.0581, -0.0485,  0.0819,  ..., -0.1148, -0.2379, -0.0197]],\n",
      "\n",
      "        [[-0.1323,  0.0316, -0.0275,  ..., -0.1664, -0.0217, -0.1073],\n",
      "         [-0.1268,  0.0755, -0.0284,  ..., -0.1989, -0.0049, -0.0052],\n",
      "         [-0.0872,  0.0954,  0.0227,  ..., -0.2042, -0.0239, -0.0679],\n",
      "         ...,\n",
      "         [-0.1429,  0.1022, -0.0314,  ..., -0.3071,  0.0589, -0.0140],\n",
      "         [-0.1263,  0.1024,  0.0184,  ..., -0.2251,  0.0540, -0.0288],\n",
      "         [-0.1557,  0.0440, -0.0579,  ..., -0.1386,  0.0082,  0.0225]],\n",
      "\n",
      "        [[ 0.3278, -0.1807,  0.3437,  ..., -0.0177,  0.1541,  0.0162],\n",
      "         [ 0.3345, -0.1769,  0.2374,  ...,  0.0912,  0.1127, -0.0584],\n",
      "         [ 0.3103, -0.1937,  0.3929,  ...,  0.0478,  0.1291, -0.0527],\n",
      "         ...,\n",
      "         [ 0.3446, -0.1568,  0.2476,  ...,  0.0884,  0.1177, -0.0636],\n",
      "         [ 0.3464, -0.1601,  0.2229,  ...,  0.1460,  0.1680, -0.1603],\n",
      "         [ 0.2861, -0.2156,  0.2918,  ...,  0.0771,  0.1391,  0.0140]]],\n",
      "       grad_fn=<BmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_input, dim_q, dim_v):\n",
    "        '''\n",
    "        dim_input: the dimension of each sample\n",
    "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
    "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
    "        '''\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_q = dim_q\n",
    "        self.dim_k = dim_q\n",
    "        self.dim_v = dim_v\n",
    "\n",
    "        # Define the linear projection\n",
    "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
    "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
    "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
    "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, n, dim_q = x.shape\n",
    "\n",
    "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
    "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
    "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
    "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
    "\n",
    "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
    "        dist = torch.softmax(dist, dim=-1)\n",
    "        print('attention matrix: ', dist.shape)\n",
    "\n",
    "        outputs = torch.bmm(dist, v)\n",
    "        print('attention outputs: ', outputs.shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "batch_size = 32 # number of samples in a batch\n",
    "dim_input = 128 # dimension of each item in the sample sequence\n",
    "seq_len = 20 # sequence length for each sample\n",
    "x = torch.randn(batch_size, seq_len, dim_input)\n",
    "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
    "\n",
    "attention = self_attention(x)\n",
    "\n",
    "print(attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_ready",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
